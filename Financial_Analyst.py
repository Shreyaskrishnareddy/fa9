"""
Python code extracted from Financial_Analyst.ipynb
Generated by Project Publisher

Original notebook: 101 cells
Code cells: 100
"""

# Imports
from bs4 import BeautifulSoup
from google.colab import files
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.llms import HuggingFaceHub
from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain_huggingface import HuggingFaceEndpoint
from langchain_openai import ChatOpenAI
from lxml import etree
from sentence_transformers import SentenceTransformer
import faiss
import fitz  # PyMuPDF
import glob
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import re
import xml.etree.ElementTree as ET

# Main code
# !pip install pandas
# !pip install xbrl
# !pip install beautifulsoup4 lxml
# !pip install pymupdf
# !pip install beautifulsoup4 lxml
# Load the HTML file
with open("/content/aapl-20230930.htm", "r", encoding="utf-8") as f:
    soup = BeautifulSoup(f, "lxml")
# Extract all text, using line breaks between tags
full_text = soup.get_text(separator="\n")
# Preview the first 1000 characters
print(full_text[:1000])
with open("/content/aapl-20230930.htm", "r", encoding="utf-8") as f:
    text_check = f.read()
print(text_check[:2000])
soup = BeautifulSoup(text_check, "lxml")
full_text = soup.get_text(separator="\n")
print(full_text[:2000])
tree = etree.parse("/content/aapl-20230930_htm.xml")
root = tree.getroot()
facts = []
for elem in root.iter():
    if "us-gaap" in elem.tag:
        facts.append({
            "Tag": elem.tag,
            "Value": elem.text,
            "ContextRef": elem.attrib.get("contextRef", None),
            "UnitRef": elem.attrib.get("unitRef", None),
        })
print("Facts extracted:", len(facts))
df_facts = pd.DataFrame(facts)
df_facts["Label"] = df_facts["Tag"].apply(lambda x: x.split("}")[1] if "}" in x else x)
print(df_facts.head(10))
df_facts["NumericValue"] = pd.to_numeric(df_facts["Value"], errors="coerce")
print(df_facts["Label"].unique())
key_metrics = df_facts[df_facts["Label"].isin([
    "Revenue",
    "NetIncomeLoss",
    "Assets",
    "Liabilities",
    "CashAndCashEquivalentsAtCarryingValue",
    "EarningsPerShareBasic",
    "EarningsPerShareDiluted"
])]
print(key_metrics[["Label", "NumericValue"]])
summary = key_metrics.pivot_table(
    index="ContextRef",
    columns="Label",
    values="NumericValue",
    aggfunc="first"
).reset_index()
print(summary)
# Build context mapping
context_dates = {}
for context in root.iter("{http://www.xbrl.org/2003/instance}context"):
    context_id = context.attrib.get("id")
    period = context.find("{http://www.xbrl.org/2003/instance}period")
    if period is not None:
        end_date_elem = period.find("{http://www.xbrl.org/2003/instance}endDate")
        instant_elem = period.find("{http://www.xbrl.org/2003/instance}instant")
        if end_date_elem is not None:
            date_str = end_date_elem.text
        elif instant_elem is not None:
            date_str = instant_elem.text
        else:
            date_str = None
        context_dates[context_id] = date_str
facts = []
for elem in root.iter():
    if "us-gaap" in elem.tag:
        context_id = elem.attrib.get("contextRef", None)
        facts.append({
            "Tag": elem.tag,
            "Value": elem.text,
            "ContextRef": context_id,
            "UnitRef": elem.attrib.get("unitRef", None),
            "Period": context_dates.get(context_id, None)
        })
df_facts = pd.DataFrame(facts)
df_facts["Label"] = df_facts["Tag"].apply(lambda x: x.split("}")[1] if "}" in x else x)
df_facts["NumericValue"] = pd.to_numeric(df_facts["Value"], errors="coerce")
print(df_facts[["Label", "NumericValue", "Period"]].head(10))
numeric_facts = df_facts[df_facts["NumericValue"].notnull()].copy()
print("Total numeric facts:", len(numeric_facts))
print(numeric_facts[["Label", "NumericValue", "Period"]].head(20))
print(sorted(numeric_facts["Label"].unique()))
key_metrics = numeric_facts[
    numeric_facts["Label"].isin([
        "RevenueFromContractWithCustomerExcludingAssessedTax",
        "NetIncomeLoss",
        "Assets",
        "Liabilities",
        "CashAndCashEquivalentsAtCarryingValue",
        "EarningsPerShareBasic",
        "EarningsPerShareDiluted",
        "ResearchAndDevelopmentExpense",
        "OperatingIncomeLoss",
        "GrossProfit"
    ])
].copy()
print(key_metrics[["Label", "NumericValue", "Period"]])
summary = key_metrics.pivot_table(
    index="Period",
    columns="Label",
    values="NumericValue",
    aggfunc="first"
).reset_index()
print(summary)
summary.to_csv("/content/apple_summary.csv", index=False)
print("âœ… Saved as apple_summary.csv")
# !pip install plotly
summary = pd.read_csv("/content/apple_summary.csv")
print(summary)
fig = px.bar(
    summary,
    x="Period",
    y="RevenueFromContractWithCustomerExcludingAssessedTax",
    title="Apple Revenue by Fiscal Year",
    labels={"RevenueFromContractWithCustomerExcludingAssessedTax": "Revenue (USD)"}
)
fig.show()
fig = px.line(
    summary,
    x="Period",
    y="NetIncomeLoss",
    title="Apple Net Income Over Time",
    markers=True,
    labels={"NetIncomeLoss": "Net Income (USD)"}
)
fig.show()
# !pip install langchain huggingface_hub
llm = HuggingFaceHub(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    model_kwargs={"temperature": 0.2, "max_new_tokens": 300},
    huggingfacehub_api_token = "YOUR_TOKEN_HERE"
)
agent = create_pandas_dataframe_agent(
    llm,
    summary,  # Your summary DataFrame
    verbose=True,
    allow_dangerous_code=True
)
response = agent.run("What was Apple's revenue in 2023?")
print(response)
# !pip install -U langchain-huggingface
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    temperature=0.2,
    max_new_tokens=300,
    huggingfacehub_api_token = "YOUR_TOKEN_HERE"
)
agent = create_pandas_dataframe_agent(
    llm,
    summary,              # Your cleaned DataFrame with metrics
    verbose=True,
    allow_dangerous_code=True
)
# !pip install openai
# !pip install -U openai langchain
os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY_HERE"
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.2
)
summary = pd.read_csv("/content/apple_summary.csv")
agent = create_pandas_dataframe_agent(
    llm,
    summary,
    verbose=True,
    allow_dangerous_code=True,
    max_iterations=15
)
response = agent.invoke("What was Apple's revenue in 2023?")
print(response)
response = agent.invoke("Summarize revenue for all years.")
print(response)
response = agent.invoke("What was the net income as a percentage of revenue in 2023?")
print(response)
response = agent.invoke("Compare net income in 2022 and 2023.")
print(response)
response = agent.invoke("Show the net income for all years.")
print(response)
def extract_text_from_pdf(pdf_path):
    text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            text += page.get_text()
    return text
# Example:
text = extract_text_from_pdf("/content/aapl-20230930.pdf")
print(text[:1000])  # Preview first 1000 chars
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
docs = splitter.create_documents([text])
# !pip install sentence-transformers faiss-cpu
embedder = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedder.encode([d.page_content for d in docs], show_progress_bar=True)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))
def search(query, k=3):
    q_embedding = embedder.encode([query])
    D, I = index.search(np.array(q_embedding), k)
    results = [docs[i].page_content for i in I[0]]
    return results
matches = search("What are the supply chain risks?")
context = "\n\n".join(matches)
prompt = f"""
You are a financial analyst.
Given the following excerpts from Apple's 10-K filing, answer the question clearly and concisely.
List the risks as numbered bullet points, each starting with a number and a period.
Example format:
1. First risk description.
2. Second risk description.
3. Third risk description.
Context:
{context}
Question: What are the supply chain risks?
"""
response = llm.invoke(prompt)
if isinstance(response, dict):
    text = response["output"]
else:
    text = response.content
split_lines = re.split(r'(\d+\.\s)', text)
chunks = []
for i in range(1, len(split_lines), 2):
    bullet = split_lines[i].strip() + split_lines[i+1].strip()
    chunks.append(bullet)
for line in chunks:
    print(line)
def semantic_answer(question, k=3):
    matches = search(question, k=k)
    context = "\n\n".join(matches)
    prompt = f"""
    You are a financial analyst.
    Given the following excerpts from Apple's 10-K filing, answer the question clearly and concisely.
    Format your answer as numbered bullet points, one per line.
    Example format:
    1. [First point]
    2. [Second point]
    3. [Third point]
    Context:
    {context}
    Question: {question}
    """
    response = llm.invoke(prompt)
    if isinstance(response, dict):
        text = response["output"]
    else:
        text = response.content
    # Split bullets reliably
    split_lines = re.split(r'(\d+\.\s)', text)
    chunks = []
    for i in range(1, len(split_lines), 2):
        bullet = split_lines[i].strip() + split_lines[i+1].strip()
        chunks.append(bullet)
    return "\n".join(chunks)
print(semantic_answer("What does Apple say about geopolitical risks?"))
def semantic_answer(question, k=3):
    matches = search(question, k=k)
    context = "\n\n".join(matches)
    prompt = f"""
    You are a financial analyst.
    Given the following excerpts from Apple's 10-K filing, answer the question clearly and concisely.
    Format your answer as numbered bullet points, each starting with a number and a period, one per line.
    Example format:
    1. [First point]
    2. [Second point]
    3. [Third point]
    Context:
    {context}
    Question: {question}
    """
    response = llm.invoke(prompt)
    # Extract clean text regardless of type
    if isinstance(response, dict):
        text = response.get("output", "")
    elif hasattr(response, "content"):
        text = response.content
    else:
        text = str(response)
    # Split into numbered bullets
    split_lines = re.split(r'(\d+\.\s)', text)
    chunks = []
    for i in range(1, len(split_lines), 2):
        bullet = split_lines[i].strip() + split_lines[i+1].strip()
        chunks.append(bullet)
    return "\n".join(chunks)
print(semantic_answer("What are Apple's competition risks?"))
print(semantic_answer("What are the key factors that could negatively impact Appleâ€™s profitability?"))
print(semantic_answer("How does Apple describe its strategy for managing supply chain disruptions?"))
# !pip install faiss-cpu sentence-transformers beautifulsoup4 lxml PyMuPDF
# Core imports
# Model
embedder = SentenceTransformer("all-MiniLM-L6-v2")
def parse_pdf(file_path):
    """Extracts text from a PDF file."""
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text
def parse_html(file_path):
    """Extracts text from an HTML/HTM file."""
    with open(file_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "lxml")
        return soup.get_text(separator="\n")
def parse_xml(file_path):
    """Extracts text from an XML file."""
    tree = ET.parse(file_path)
    root = tree.getroot()
    text = ET.tostring(root, encoding="unicode", method="text")
    return text
def chunk_text(text, chunk_size=500, overlap=50):
    """
    Splits text into overlapping chunks.
    """
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks
def embed_chunks(chunks):
    """
    Converts text chunks to embeddings using sentence-transformers.
    """
    embeddings = embedder.encode(
        chunks,
        convert_to_numpy=True,
        show_progress_bar=True
    )
    return embeddings
# Upload multiple files
uploaded_files = files.upload()
# Map file extensions to parser functions
parser_map = {
    ".pdf": parse_pdf,
    ".htm": parse_html,
    ".html": parse_html,
    ".xml": parse_xml
}
# Helper to choose parser based on extension
def get_parser(file_name):
    ext = os.path.splitext(file_name)[1].lower()
    return parser_map.get(ext, None)
all_chunks = []
all_embeddings = []
all_metadata = []
for file_name in uploaded_files.keys():
    parser = get_parser(file_name)
    if parser is None:
        print(f"âš ï¸ Skipping unsupported file: {file_name}")
        continue
    print(f"âœ… Processing: {file_name}")
    # Parse text
    text = parser(file_name)
    # Chunk text
    chunks = chunk_text(text, chunk_size=500, overlap=50)
    # Embed chunks
    embeddings = embed_chunks(chunks)
    # Collect
    all_chunks.extend(chunks)
    all_embeddings.append(embeddings)
    # Store metadata for each chunk
    all_metadata.extend([
        {"file_name": file_name, "chunk_id": idx}
        for idx in range(len(chunks))
    ])
print("âœ… All files processed.")
# Stack all embeddings into one big matrix
embedding_matrix = np.vstack(all_embeddings)
# Create FAISS index
dimension = embedding_matrix.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embedding_matrix)
print(f"âœ… FAISS index created with {index.ntotal} vectors.")
def semantic_search(query, k=5):
    """
    Embeds the query and retrieves top-k most similar chunks with metadata.
    """
    # Embed the query
    q_embedding = embedder.encode([query])
    # Search FAISS
    distances, indices = index.search(np.array(q_embedding), k)
    # Collect results
    results = []
    for idx in indices[0]:
        result = {
            "text": all_chunks[idx],
            "metadata": all_metadata[idx]
        }
        results.append(result)
    return results
def display_results(results):
    """
    Pretty-print retrieved chunks with metadata.
    """
    for i, r in enumerate(results, start=1):
        print(f"\nðŸ”¹ Result #{i}")
        print(f"ðŸ“„ File: {r['metadata']['file_name']} | Chunk ID: {r['metadata']['chunk_id']}")
        print("------")
        snippet = r["text"][:500]  # Show only first 500 characters
        print(snippet)
# Example query
query = "What are Apple's supply chain risks?"
# Run search
results = semantic_search(query, k=5)
# Display results
display_results(results)
# Initialize your LLM (change model if needed)
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.2
)
def summarize_with_llm(query, results):
    """
    Combines retrieved chunks and sends them to the LLM to generate an answer.
    """
    # Build context with citations
    context = ""
    for r in results:
        snippet = r["text"][:1000].strip().replace("\n", " ")
        file = r["metadata"]["file_name"]
        context += f"\n(Source: {file})\n{snippet}\n"
    # Prompt
    prompt = (
        f"You are a financial analyst assistant.\n\n"
        f"Context excerpts:\n{context}\n\n"
        f"Question: {query}\n\n"
        f"Answer in bullet points with clear sentences."
    )
    response = llm.invoke(prompt)
    return response.content.strip()
# Example question
query = "Summarize Apple's supply chain risks."
# Retrieve top 5 chunks
results = semantic_search(query, k=5)
# Generate answer
summary = summarize_with_llm(query, results)
# Display answer
print("âœ… LLM Summary:\n")
print(summary)
# Example 2
query2 = "What were the key financial highlights in 2023?"
results2 = semantic_search(query2, k=5)
summary2 = summarize_with_llm(query2, results2)
print("âœ… Financial Highlights Summary:\n")
print(summary2)
def extract_xbrl_facts(xml_file):
    """
    Extracts all us-gaap facts and their values from an XBRL XML file.
    Returns a list of dicts with label and value.
    """
    tree = etree.parse(xml_file)
    root = tree.getroot()
    ns = {"us-gaap": "http://fasb.org/us-gaap/2023"}
    # Find all tags in the us-gaap namespace
    facts = []
    for element in root.iter():
        if element.tag.startswith("{http://fasb.org/us-gaap/2023}"):
            tag = element.tag.split("}")[1]
            value = element.text
            facts.append({
                "Label": tag,
                "Value": value
            })
    return facts
# Path to your uploaded XBRL file
xbrl_file = "aapl-20230930_htm.xml"
# Extract all facts
facts = extract_xbrl_facts(xbrl_file)
# Show a sample of what we got
print(f"âœ… Extracted {len(facts)} facts. Showing first 10:\n")
for f in facts[:10]:
    print(f)
# Define the tags we care about
important_tags = {
    "RevenueFromContractWithCustomerExcludingAssessedTax": "Revenue",
    "Revenues": "Revenue",
    "SalesRevenueNet": "Revenue",
    "CostOfGoodsAndServicesSold": "COGS",
    "OperatingIncomeLoss": "OperatingIncome",
    "NetIncomeLoss": "NetIncome",
    "Assets": "TotalAssets",
    "Liabilities": "TotalLiabilities",
    "StockholdersEquity": "Equity",
    "LongTermDebt": "LongTermDebt",
}
# Initialize empty dict
data = {}
# Loop through extracted facts
for f in facts:
    tag = f["Label"]
    value = f["Value"]
    if tag in important_tags:
        try:
            # Convert to numeric safely
            numeric_value = float(value)
            data[important_tags[tag]] = numeric_value
        except:
            continue
# Display cleaned data
print("âœ… Extracted Financial Metrics:\n")
for k, v in data.items():
    print(f"{k}: {v:,.0f}")
# Convert dict to DataFrame
df = pd.DataFrame([data])
# Compute Ratios
df["GrossMargin"] = (df["Revenue"] - df["COGS"]) / df["Revenue"] * 100
df["OperatingMargin"] = df["OperatingIncome"] / df["Revenue"] * 100
df["NetMargin"] = df["NetIncome"] / df["Revenue"] * 100
df["DebtEquity"] = df["LongTermDebt"] / df["Equity"]
df["LiabilitiesEquity"] = df["TotalLiabilities"] / df["Equity"]
# Display nicely
print("âœ… Financial Ratios:\n")
print(df[[
    "GrossMargin",
    "OperatingMargin",
    "NetMargin",
    "DebtEquity",
    "LiabilitiesEquity"
]].round(2))
# Create figure and axes
fig, ax = plt.subplots(figsize=(8, 6))
# Bar plot of the ratios
bars = ax.bar(
    ["Gross Margin", "Operating Margin", "Net Margin"],
    [
        df["GrossMargin"].iloc[0],
        df["OperatingMargin"].iloc[0],
        df["NetMargin"].iloc[0]
    ],
    color="#5fba7d"
)
# Add value labels
for bar in bars:
    yval = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f"{yval:.2f}%", ha='center', fontsize=12)
# Customize plot
ax.set_title("Apple Financial Margins (%)", fontsize=16)
ax.set_ylabel("Percentage")
ax.set_ylim(0, max(df["GrossMargin"].iloc[0], df["OperatingMargin"].iloc[0], df["NetMargin"].iloc[0]) + 10)
plt.tight_layout()
# Show plot
plt.show()
file = "/content/aapl-20210925_htm.xml"
with open(file, "r") as f:
    soup = BeautifulSoup(f, "lxml")
# List unique tag names
all_tags = set([tag.name for tag in soup.find_all()])
print("âœ… Unique tag names in 2021 file:")
for t in sorted(all_tags):
    print(t)
uploaded = files.upload()
# Only pick Apple filings ending with _htm.xml
files = sorted(glob.glob("aapl-*_htm.xml"))
print(f"âœ… Found {len(files)} files:")
for f in files:
    print(" -", f)
# Collect only aapl-*_htm.xml files in current directory
xml_files = sorted(glob.glob("aapl-*_htm.xml"))
if not xml_files:
    print("âš ï¸ No matching files found.")
else:
    print(f"âœ… Found {len(xml_files)} files:")
    for f in xml_files:
        print(f" - {f}")
# Dictionary to hold all tags per file
all_file_tags = {}
# Process each file
for file in xml_files:
    with open(file, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "xml")
    # Get all unique tag names
    tags = sorted({tag.name for tag in soup.find_all()})
    all_file_tags[file] = tags
# Display all tags found in each file
for file, tags in all_file_tags.items():
    print(f"\nâœ… {file} has {len(tags)} unique tags:")
    print(tags)
# Prepare list of tags we care about
kpi_tags = {
    "Revenue": "RevenueFromContractWithCustomerExcludingAssessedTax",
    "NetIncome": "NetIncomeLoss",
    "Assets": "Assets",
    "Liabilities": "Liabilities",
    "Equity": "StockholdersEquity",
    "LongTermDebt": "LongTermDebtNoncurrent"
}
# Collect only aapl-*_htm.xml files
xml_files = sorted(glob.glob("aapl-*_htm.xml"))
if not xml_files:
    print("âš ï¸ No matching files found.")
else:
    print(f"âœ… Found {len(xml_files)} files to process:")
    for f in xml_files:
        print(f" - {f}")
# List to hold data rows
data = []
# Process each file
for file in xml_files:
    with open(file, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "xml")
    row = {"File": file}
    for kpi, tag in kpi_tags.items():
        element = soup.find(tag)
        value = element.text.strip() if element and element.text else None
        row[kpi] = value
    data.append(row)
# Convert to DataFrame
df = pd.DataFrame(data)
# Display the DataFrame
df
# Prepare list of tags we care about
kpi_tags = {
    "Revenue": "RevenueFromContractWithCustomerExcludingAssessedTax",
    "NetIncome": "NetIncomeLoss",
    "Assets": "Assets",
    "Liabilities": "Liabilities",
    "Equity": "StockholdersEquity",
    "LongTermDebt": "LongTermDebtNoncurrent",
    "FiscalYear": "DocumentFiscalYearFocus"
}
# Collect only aapl-*_htm.xml files
xml_files = sorted(glob.glob("aapl-*_htm.xml"))
data = []
for file in xml_files:
    with open(file, "r") as f:
        soup = BeautifulSoup(f, "xml")
    row = {"File": file}
    for kpi, tag in kpi_tags.items():
        element = soup.find(tag)
        if element:
            raw_value = element.text.strip()
            clean_value = raw_value.replace(",", "").replace("$", "")
            try:
                numeric_value = float(clean_value)
            except ValueError:
                numeric_value = None
        else:
            numeric_value = None
        row[kpi] = numeric_value
    data.append(row)
df = pd.DataFrame(data)
df
# Convert KPI columns to numeric
kpi_cols = ["Revenue", "NetIncome", "Assets", "Liabilities", "Equity"]
for col in kpi_cols:
    df[col] = pd.to_numeric(df[col], errors="coerce")
# Extract year
df["Year"] = df["File"].str.extract(r"(\d{4})").astype(int)
df_sorted = df.sort_values("Year")
# Create the figure
fig = go.Figure()
# Custom color palette
colors = ["#1f77b4", "#2ca02c", "#d62728"]
# Add one bar trace per year
for i, (_, row) in enumerate(df_sorted.iterrows()):
    fig.add_trace(
        go.Bar(
            x=kpi_cols,
            y=[row[kpi] for kpi in kpi_cols],
            name=str(row["Year"]),
            marker_color=colors[i]
        )
    )
# Update layout for clarity
fig.update_layout(
    title="Apple Financial KPIs by Year",
    xaxis_title="KPI",
    yaxis_title="Amount (USD Millions)",
    barmode="group",
    legend_title="Fiscal Year",
    template="plotly_white",
    font=dict(size=13)
)
fig.show()

if __name__ == "__main__":
    # Run the main functionality
    print("Notebook code extracted successfully")